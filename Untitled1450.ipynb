{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6db510-b955-49f8-941c-1872cf6fa9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63fd67c55a04a5d876c5407d9d21dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sagni\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-Coder-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cfc7df723b489a9f3766c4b80eaf17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a94c3e074024b2ab87e8f54162caa94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfce93ffcb2a467593bce57f0e4415b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa65dacda53c431fbd7c3c80b6a90a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Temp\\ipykernel_31204\\1497219379.py\", line 136, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Temp\\ipykernel_31204\\1497219379.py\", line 133, in main\n",
      "    generate_code(question, save_to_file=True)\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Temp\\ipykernel_31204\\1497219379.py\", line 87, in generate_code\n",
      "    tokenizer, model = load_model_and_tokenizer()\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Temp\\ipykernel_31204\\1497219379.py\", line 76, in load_model_and_tokenizer\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 597, in from_pretrained\n",
      "    model_class = _get_model_class(config, cls._model_mapping)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 394, in _get_model_class\n",
      "    supported_models = model_mapping[type(config)]\n",
      "                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 803, in __getitem__\n",
      "    return self._load_attr_from_module(model_type, model_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 817, in _load_attr_from_module\n",
      "    return getattribute_from_module(self._modules[module_name], attr)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 729, in getattribute_from_module\n",
      "    if hasattr(module, attr):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2154, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2184, in _get_module\n",
      "    raise e\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2182, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py\", line 27, in <module>\n",
      "    from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 73, in <module>\n",
      "    from .loss.loss_utils import LOSS_MAPPING\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\loss\\loss_utils.py\", line 21, in <module>\n",
      "    from .loss_d_fine import DFineForObjectDetectionLoss\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\loss\\loss_d_fine.py\", line 21, in <module>\n",
      "    from .loss_for_object_detection import (\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\loss\\loss_for_object_detection.py\", line 32, in <module>\n",
      "    from transformers.image_transforms import center_to_corners_format\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_transforms.py\", line 48, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\", line 11, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import distribute\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__.distribute import combinations\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.distribute.combinations import env # line: 456\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py\", line 33, in <module>\n",
      "    from tensorflow.python.distribute import collective_all_reduce_strategy\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\", line 30, in <module>\n",
      "    from tensorflow.python.distribute import input_lib\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1032, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1131, in get_data\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2176, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1182, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1053, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 861, in structured_traceback\n",
      "    formatted_exceptions: list[list[str]] = self.format_exception_as_a_whole(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 746, in format_exception_as_a_whole\n",
      "    records = self.get_records(etb, context, tb_offset) if etb else []\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 819, in get_records\n",
      "    mod = inspect.getmodule(cf.tb_frame)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py\", line 980, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py\", line 963, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py\", line 948, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py\", line 987, in getmodule\n",
      "    for modname, module in sys.modules.copy().items():\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "import os, re, sys, json, torch\n",
    "from typing import Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ---------------- USER KNOBS ----------------\n",
    "BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"Qwen/Qwen2.5-Coder-0.5B-Instruct\")\n",
    "MAX_NEW_TOKENS = int(os.environ.get(\"MAX_NEW_TOKENS\", \"512\"))\n",
    "TEMPERATURE = float(os.environ.get(\"TEMPERATURE\", \"0.2\"))\n",
    "TOP_P = float(os.environ.get(\"TOP_P\", \"0.95\"))\n",
    "# -------------------------------------------\n",
    "\n",
    "def try_bitsandbytes_cfg():\n",
    "    \"\"\"Return a BitsAndBytesConfig if available and CUDA exists; else None.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_prompt(tokenizer, question: str) -> str:\n",
    "    \"\"\"Use chat template if the model has one; fallback to simple instruction format.\"\"\"\n",
    "    user = (\n",
    "        \"You are a helpful coding assistant. \"\n",
    "        \"Write correct, secure, and concise code for the following task.\\n\\n\"\n",
    "        f\"Task:\\n{question}\\n\\n\"\n",
    "        \"Return ONLY code inside a triple-backtick block.\"\n",
    "    )\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": user}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    except Exception:\n",
    "        return f\"### Instruction:\\n{user}\\n\\n### Response:\\n\"\n",
    "\n",
    "CODE_BLOCK_RE = re.compile(\n",
    "    r\"```([a-zA-Z0-9_+-]*)\\s*\\n(.*?)```\",\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "def extract_code(text: str) -> (Optional[str], Optional[str]):\n",
    "    \"\"\"Return (language, code) from the first fenced block, if any.\"\"\"\n",
    "    m = CODE_BLOCK_RE.search(text)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    lang = (m.group(1) or \"\").strip() or None\n",
    "    code = m.group(2).strip(\"\\n\\r \")\n",
    "    return lang, code\n",
    "\n",
    "def suggest_ext(lang: Optional[str]) -> str:\n",
    "    mapping = {\n",
    "        \"py\":\"py\",\"python\":\"py\",\n",
    "        \"js\":\"js\",\"javascript\":\"js\",\"ts\":\"ts\",\"typescript\":\"ts\",\n",
    "        \"java\":\"java\",\"cpp\":\"cpp\",\"c\":\"c\",\"c++\":\"cpp\",\"cs\":\"cs\",\"csharp\":\"cs\",\n",
    "        \"go\":\"go\",\"rs\":\"rs\",\"rust\":\"rs\",\"php\":\"php\",\"rb\":\"rb\",\"ruby\":\"rb\",\n",
    "        \"sh\":\"sh\",\"bash\":\"sh\",\"ps1\":\"ps1\",\"powershell\":\"ps1\",\n",
    "        \"kt\":\"kt\",\"kotlin\":\"kt\",\"swift\":\"swift\",\"sql\":\"sql\",\"html\":\"html\",\"css\":\"css\"\n",
    "    }\n",
    "    if not lang: return \"txt\"\n",
    "    return mapping.get(lang.lower(), \"txt\")\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    qcfg = try_bitsandbytes_cfg()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=qcfg,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_code(question: str, save_to_file: bool = True) -> str:\n",
    "    tokenizer, model = load_model_and_tokenizer()\n",
    "    prompt = build_prompt(tokenizer, question)\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    full = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # Remove prompt prefix if present\n",
    "    if full.startswith(prompt):\n",
    "        full = full[len(prompt):]\n",
    "\n",
    "    lang, code = extract_code(full)\n",
    "    if code is None:\n",
    "        # fallback: return raw completion if the model didnâ€™t fence code\n",
    "        code = full.strip()\n",
    "        lang = None\n",
    "\n",
    "    print(\"\\n===== GENERATED CODE =====\\n\")\n",
    "    print(code)\n",
    "    print(\"\\n==========================\\n\")\n",
    "\n",
    "    if save_to_file:\n",
    "        ext = suggest_ext(lang)\n",
    "        fname = f\"generated_code.{ext}\"\n",
    "        with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(code)\n",
    "        print(f\"[Saved] {fname}\")\n",
    "\n",
    "    return code\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) > 1:\n",
    "        question = \" \".join(sys.argv[1:]).strip()\n",
    "    else:\n",
    "        question = input(\"Enter your coding question/task:\\n> \").strip()\n",
    "    if not question:\n",
    "        print(\"No question provided.\")\n",
    "        sys.exit(1)\n",
    "    generate_code(question, save_to_file=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4aab7-2cd4-4586-a0dd-3ca5538734a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
