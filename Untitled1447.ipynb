{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330a2308-d2db-49f6-8d5e-910559e66abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CodeGenerator Artifacts Written ===\n",
      "H5:    C:\\Users\\sagni\\Downloads\\Code Generator\\codegen.h5\n",
      "PKL:   C:\\Users\\sagni\\Downloads\\Code Generator\\codegen.pkl\n",
      "YAML:  C:\\Users\\sagni\\Downloads\\Code Generator\\codegen_config.yaml\n",
      "JSONL: C:\\Users\\sagni\\Downloads\\Code Generator\\codegen.jsonl\n",
      "SUM:   C:\\Users\\sagni\\Downloads\\Code Generator\\codegen_summary.json\n",
      "\n",
      "Sizes: {'train': 18612}\n",
      "Columns used: {'instruction_column_used': 'instruction', 'input_column_used': 'input', 'output_column_used': 'output'}\n",
      "Stats: {\n",
      "  \"num_examples\": 18612,\n",
      "  \"instruction_chars\": {\n",
      "    \"min\": 21,\n",
      "    \"max\": 7755,\n",
      "    \"mean\": 98.19,\n",
      "    \"median\": 83.0\n",
      "  },\n",
      "  \"input_chars\": {\n",
      "    \"min\": 1,\n",
      "    \"max\": 800,\n",
      "    \"mean\": 29.24,\n",
      "    \"median\": 14.0\n",
      "  },\n",
      "  \"output_chars\": {\n",
      "    \"min\": 1,\n",
      "    \"max\": 20778,\n",
      "    \"mean\": 465.94,\n",
      "    \"median\": 260.0\n",
      "  },\n",
      "  \"output_lines\": {\n",
      "    \"min\": 1,\n",
      "    \"max\": 542,\n",
      "    \"mean\": 16.88,\n",
      "    \"median\": 11.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "    HAVE_YAML = True\n",
    "except Exception:\n",
    "    HAVE_YAML = False\n",
    "\n",
    "# --------- USER PATHS ----------\n",
    "CSV_PATH = r\"C:\\Users\\sagni\\Downloads\\Code Generator\\archive\\train.csv\"\n",
    "OUT_DIR  = r\"C:\\Users\\sagni\\Downloads\\Code Generator\"\n",
    "# --------------------------------\n",
    "\n",
    "# Case-insensitive candidate names\n",
    "INSTR_CANDS = [\n",
    "    \"instruction\",\"prompt\",\"question\",\"task\",\"query\",\n",
    "    \"title\",\"description\",\"desc\",\"nl\",\"problem\",\"user\",\"spec\"\n",
    "]\n",
    "INPUT_CANDS = [\n",
    "    \"input\",\"context\",\"constraints\",\"examples\",\"params\",\"stdin\",\"additional_input\"\n",
    "]\n",
    "OUTPUT_CANDS = [\n",
    "    \"output\",\"answer\",\"solution\",\"completion\",\"code\",\"response\",\"target\",\"program\",\"result\"\n",
    "]\n",
    "\n",
    "def ensure_out_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def _read_csv_any(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=\"latin-1\")\n",
    "        except Exception:\n",
    "            return pd.read_csv(path, engine=\"python\")\n",
    "\n",
    "def _trim_keep_code(s: Optional[str]) -> str:\n",
    "    # Only strip ends; DO NOT collapse internal whitespace/newlines\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return str(s).strip()\n",
    "\n",
    "def detect_columns(df: pd.DataFrame) -> Tuple[pd.Series, pd.Series, pd.Series, Dict[str,str]]:\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    instr_col = next((lower[c] for c in INSTR_CANDS if c in lower), None)\n",
    "    input_col = next((lower[c] for c in INPUT_CANDS if c in lower), None)\n",
    "    output_col = next((lower[c] for c in OUTPUT_CANDS if c in lower), None)\n",
    "\n",
    "    if instr_col is None or output_col is None:\n",
    "        if df.shape[1] >= 2:\n",
    "            cols = list(df.columns)\n",
    "            instr_col = instr_col or cols[0]\n",
    "            output_col = output_col or cols[1]\n",
    "        elif df.shape[1] == 1:\n",
    "            instr_col = instr_col or df.columns[0]\n",
    "            output_col = output_col or None\n",
    "        else:\n",
    "            obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "            if len(obj_cols) >= 2:\n",
    "                instr_col = instr_col or obj_cols[0]\n",
    "                output_col = output_col or obj_cols[1]\n",
    "            elif len(obj_cols) == 1:\n",
    "                instr_col = instr_col or obj_cols[0]\n",
    "                output_col = output_col or None\n",
    "\n",
    "    instr = df[instr_col].map(_trim_keep_code) if instr_col else pd.Series([], dtype=str)\n",
    "\n",
    "    if input_col:\n",
    "        inp = df[input_col].map(_trim_keep_code)\n",
    "    else:\n",
    "        remaining = [c for c in df.columns if c not in {instr_col, output_col}]\n",
    "        inp = df[remaining[0]].map(_trim_keep_code) if remaining else pd.Series([\"\"] * len(df))\n",
    "\n",
    "    out = df[output_col].map(_trim_keep_code) if output_col else pd.Series([\"\"] * len(df))\n",
    "\n",
    "    n = min(len(instr), len(inp), len(out))\n",
    "    instr, inp, out = instr.iloc[:n].reset_index(drop=True), inp.iloc[:n].reset_index(drop=True), out.iloc[:n].reset_index(drop=True)\n",
    "\n",
    "    mask = (instr != \"\") & (out != \"\")\n",
    "    instr, inp, out = instr[mask].reset_index(drop=True), inp[mask].reset_index(drop=True), out[mask].reset_index(drop=True)\n",
    "\n",
    "    names_used = {\n",
    "        \"instruction_column_used\": instr_col or (df.columns[0] if len(df.columns) else \"\"),\n",
    "        \"input_column_used\": input_col or \"\",\n",
    "        \"output_column_used\": output_col or (df.columns[1] if len(df.columns) > 1 else \"\")\n",
    "    }\n",
    "    return instr, inp, out, names_used\n",
    "\n",
    "def write_h5(path: str, instr: pd.Series, inp: pd.Series, out: pd.Series) -> None:\n",
    "    with h5py.File(path, \"w\") as h5:\n",
    "        grp = h5.create_group(\"train\")\n",
    "        str_dt = h5py.string_dtype(encoding=\"utf-8\")\n",
    "        grp.create_dataset(\"instruction\", data=instr.astype(str).values, dtype=str_dt, compression=\"gzip\")\n",
    "        grp.create_dataset(\"input\",       data=inp.astype(str).values,   dtype=str_dt, compression=\"gzip\")\n",
    "        grp.create_dataset(\"output\",      data=out.astype(str).values,   dtype=str_dt, compression=\"gzip\")\n",
    "\n",
    "def write_pkl(path: str, df: pd.DataFrame, meta: Dict[str, Any]) -> None:\n",
    "    payload = {\"data\": df.copy(), \"meta\": meta}\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def write_yaml(path: str, meta: Dict[str, Any]) -> None:\n",
    "    if HAVE_YAML:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            yaml.safe_dump(meta, f, sort_keys=False, allow_unicode=True)\n",
    "    else:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(meta, ensure_ascii=False, indent=2))\n",
    "\n",
    "def write_jsonl(path: str, instr: pd.Series, inp: pd.Series, out: pd.Series) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for a, b, c in zip(instr, inp, out):\n",
    "            rec = {\"instruction\": a, \"input\": b, \"output\": c}\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def basic_stats(instr: pd.Series, inp: pd.Series, out: pd.Series) -> Dict[str, Any]:\n",
    "    def char_stats(s: pd.Series) -> Dict[str, float]:\n",
    "        lens = s.map(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "        return {\n",
    "            \"min\": int(lens.min()) if len(lens) else 0,\n",
    "            \"max\": int(lens.max()) if len(lens) else 0,\n",
    "            \"mean\": float(round(lens.mean(), 2)) if len(lens) else 0.0,\n",
    "            \"median\": float(round(lens.median(), 2)) if len(lens) else 0.0\n",
    "        }\n",
    "    def line_stats(s: pd.Series) -> Dict[str, float]:\n",
    "        lines = s.map(lambda x: x.count(\"\\n\") + 1 if isinstance(x, str) and x != \"\" else 0)\n",
    "        return {\n",
    "            \"min\": int(lines.min()) if len(lines) else 0,\n",
    "            \"max\": int(lines.max()) if len(lines) else 0,\n",
    "            \"mean\": float(round(lines.mean(), 2)) if len(lines) else 0.0,\n",
    "            \"median\": float(round(lines.median(), 2)) if len(lines) else 0.0\n",
    "        }\n",
    "    return {\n",
    "        \"num_examples\": int(len(instr)),\n",
    "        \"instruction_chars\": char_stats(instr),\n",
    "        \"input_chars\": char_stats(inp),\n",
    "        \"output_chars\": char_stats(out),\n",
    "        \"output_lines\": line_stats(out)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    ensure_out_dir(OUT_DIR)\n",
    "\n",
    "    df_raw = _read_csv_any(CSV_PATH)\n",
    "    instr, inp, out, names_used = detect_columns(df_raw)\n",
    "\n",
    "    data = pd.DataFrame({\"instruction\": instr.astype(str), \"input\": inp.astype(str), \"output\": out.astype(str)})\n",
    "\n",
    "    stats = basic_stats(instr, inp, out)\n",
    "    meta = {\n",
    "        \"dataset_name\": \"codegen_single_csv\",\n",
    "        \"source_csv\": CSV_PATH,\n",
    "        \"created_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"columns_used\": names_used,\n",
    "        \"sizes\": {\"train\": int(len(data))},\n",
    "        \"stats\": stats,\n",
    "        \"schema\": {\"fields\": [\"instruction\", \"input\", \"output\"]}\n",
    "    }\n",
    "\n",
    "    out_h5    = os.path.join(OUT_DIR, \"codegen.h5\")\n",
    "    out_pkl   = os.path.join(OUT_DIR, \"codegen.pkl\")\n",
    "    out_yaml  = os.path.join(OUT_DIR, \"codegen_config.yaml\")\n",
    "    out_jsonl = os.path.join(OUT_DIR, \"codegen.jsonl\")\n",
    "    out_sum   = os.path.join(OUT_DIR, \"codegen_summary.json\")\n",
    "\n",
    "    write_h5(out_h5, instr, inp, out)\n",
    "    write_pkl(out_pkl, data, meta)\n",
    "    write_yaml(out_yaml, meta)\n",
    "    write_jsonl(out_jsonl, instr, inp, out)\n",
    "    with open(out_sum, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"=== CodeGenerator Artifacts Written ===\")\n",
    "    print(f\"H5:    {out_h5}\")\n",
    "    print(f\"PKL:   {out_pkl}\")\n",
    "    print(f\"YAML:  {out_yaml}\")\n",
    "    print(f\"JSONL: {out_jsonl}\")\n",
    "    print(f\"SUM:   {out_sum}\")\n",
    "    print(\"\\nSizes:\", meta[\"sizes\"])\n",
    "    print(\"Columns used:\", meta[\"columns_used\"])\n",
    "    print(\"Stats:\", json.dumps(stats, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f7240-507d-4c68-b480-ed28cc4f353d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
